{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGwkAAOCBw8A"
   },
   "source": [
    "# Installations and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "akQ9XMYfbovG",
    "outputId": "06bcb79a-f3ac-48ec-ae52-afb0a5fd1908"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting tf-keras-vis\n",
      "  Downloading tf_keras_vis-0.8.1-py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 2.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from tf-keras-vis) (2.9.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tf-keras-vis) (21.3)\n",
      "Collecting deprecated\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from tf-keras-vis) (1.7.3)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from tf-keras-vis) (4.12.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from tf-keras-vis) (7.1.2)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated->tf-keras-vis) (1.14.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio->tf-keras-vis) (1.21.6)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->tf-keras-vis) (4.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->tf-keras-vis) (3.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tf-keras-vis) (3.0.9)\n",
      "Installing collected packages: deprecated, tf-keras-vis\n",
      "Successfully installed deprecated-1.2.13 tf-keras-vis-0.8.1\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.17.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 35.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n",
      "Installing collected packages: tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.17.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras-vis\n",
    "!pip install tensorflow-addons\n",
    "\n",
    "# Imports\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import shutil\n",
    "import random\n",
    "import zipfile\n",
    "from imutils import paths\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.applications import MobileNetV2, ResNet50, VGG16, InceptionV3, Xception\n",
    "from tensorflow.keras.layers import AveragePooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from tf_keras_vis.utils.scores import BinaryScore\n",
    "from tf_keras_vis.saliency import Saliency\n",
    "from tf_keras_vis.gradcam import Gradcam\n",
    "from tf_keras_vis.gradcam_plus_plus import GradcamPlusPlus\n",
    "from tf_keras_vis.scorecam import Scorecam\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TavJNBt5p4GX"
   },
   "source": [
    "# Create train, validation and test data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "_NEnBR_6BVQn"
   },
   "outputs": [],
   "source": [
    "#@title function to create resnet50\n",
    "\n",
    "from gc import callbacks\n",
    "\n",
    "def model_resnet50():\n",
    "  IMG_SIZE = (224,224)\n",
    "  IMG_SHAPE = IMG_SIZE + (3,)\n",
    "\n",
    "  preprocess_input = tf.keras.applications.resnet50.preprocess_input\n",
    "\n",
    "  inputs = tf.keras.Input(shape=(IMG_SHAPE))\n",
    "  x = preprocess_input(inputs)\n",
    "\n",
    "  base_model = ResNet50(input_shape=(IMG_SHAPE),\n",
    "                          include_top=False,\n",
    "                          weights='imagenet',\n",
    "                        input_tensor=x)\n",
    "\n",
    "  # base_model.trainable = False\n",
    "  base_model.trainable = True\n",
    "\n",
    "  if base_model.trainable:\n",
    "      # Let's take a look to see how many layers are in the base model\n",
    "    print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "    # Fine-tune from this layer onwards\n",
    "    fine_tune_at = 175\n",
    "    \n",
    "    number_frozen = 0\n",
    "\n",
    "    # Freeze all the layers before the `fine_tune_at` layer\n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "      layer.trainable = False\n",
    "      number_frozen = number_frozen + 1\n",
    "\n",
    "    # Un-freeze the BatchNorm layers\n",
    "    for layer in base_model.layers:\n",
    "      if \"BatchNormalization\" in layer.__class__.__name__:\n",
    "          layer.trainable = True\n",
    "          number_frozen = number_frozen - 1\n",
    "\n",
    "    print(str(number_frozen) + \" layers were frozen\")\n",
    "    print(str(len(base_model.layers) - number_frozen) + \" layers to fine-tune\")\n",
    "\n",
    "\n",
    "\n",
    "  # inputs = tf.keras.Input(shape=(IMG_SHAPE))\n",
    "  # x = preprocess_input(inputs)\n",
    "  # x = base_model(x, training=base_model.trainable)\n",
    "  # x = tf.keras.layers.AveragePooling2D()(base_model.output)\n",
    "  x = tf.keras.layers.Flatten()(base_model.output)\n",
    "  x = tf.keras.layers.Dropout(0.5)(x)\n",
    "  # x = tf.keras.layers.Dense(2048, activation=\"relu\")(x)\n",
    "  x = tf.keras.layers.Dense(100, activation=\"relu\")(x)\n",
    "  # x = tf.keras.layers.Dropout(0.5)(x)\n",
    "  x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "  # x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "  outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "  model = tf.keras.Model(base_model.input, outputs)\n",
    "\n",
    "\n",
    "  base_learning_rate = 0.0003\n",
    "  #import tensorflow_addons as tfa\n",
    "  opt = tf.optimizers.Adam(learning_rate=base_learning_rate, decay=base_learning_rate/50)\n",
    "\n",
    "\n",
    "  model.compile(optimizer=opt,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "  # model.compile(optimizer='SGD',\n",
    "  #               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "  #               metrics=['accuracy'])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "DPv2keYE7eGS"
   },
   "outputs": [],
   "source": [
    "#@title Function to create MobileNet\n",
    "\n",
    "from gc import callbacks\n",
    "\n",
    "def model_mobilenetv2():\n",
    "  IMG_SIZE = (224,224)\n",
    "  IMG_SHAPE = IMG_SIZE + (3,)\n",
    "\n",
    "  preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "\n",
    "  inputs = tf.keras.Input(shape=(IMG_SHAPE))\n",
    "  x = preprocess_input(inputs)\n",
    "  \n",
    "  base_model = MobileNetV2(input_shape=(IMG_SHAPE),\n",
    "                         include_top=False,\n",
    "                         weights='imagenet',\n",
    "                         input_tensor=x)\n",
    "\n",
    "  # base_model.trainable = False\n",
    "  base_model.trainable = True\n",
    "\n",
    "  if base_model.trainable:\n",
    "      # Let's take a look to see how many layers are in the base model\n",
    "    print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "    # Fine-tune from this layer onwards\n",
    "    fine_tune_at = 100\n",
    "    \n",
    "    number_frozen = 0\n",
    "\n",
    "    # Freeze all the layers before the `fine_tune_at` layer\n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "      layer.trainable = False\n",
    "      number_frozen = number_frozen + 1\n",
    "\n",
    "    # Un-freeze the BatchNorm layers\n",
    "    for layer in base_model.layers:\n",
    "      if \"BatchNormalization\" in layer.__class__.__name__:\n",
    "          layer.trainable = True\n",
    "          number_frozen = number_frozen - 1\n",
    "\n",
    "    print(str(number_frozen) + \" layers were frozen\")\n",
    "    print(str(len(base_model.layers) - number_frozen) + \" layers to fine-tune\")\n",
    "\n",
    "\n",
    "\n",
    "  # inputs = tf.keras.Input(shape=(IMG_SHAPE))\n",
    "  # x = preprocess_input(inputs)\n",
    "  # x = base_model(x, training=base_model.trainable)\n",
    "  # x = tf.keras.layers.AveragePooling2D()(base_model.output)\n",
    "  x = tf.keras.layers.Flatten()(base_model.output)\n",
    "  x = tf.keras.layers.Dropout(0.5)(x)\n",
    "  # x = tf.keras.layers.Dense(2048, activation=\"relu\")(x)\n",
    "  x = tf.keras.layers.Dense(100, activation=\"relu\")(x)\n",
    "  # x = tf.keras.layers.Dropout(0.5)(x)\n",
    "  x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "  # x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "  outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "  model = tf.keras.Model(base_model.input, outputs)\n",
    "\n",
    "\n",
    "  base_learning_rate = 0.0003\n",
    "  #import tensorflow_addons as tfa\n",
    "  opt = tf.optimizers.Adam(learning_rate=base_learning_rate, decay=base_learning_rate/50)\n",
    "\n",
    "\n",
    "  model.compile(optimizer=opt,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "  # model.compile(optimizer='SGD',\n",
    "  #               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "  #               metrics=['accuracy'])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "yLJmMUSBZmBf"
   },
   "outputs": [],
   "source": [
    "#@title function to create vgg16\n",
    "\n",
    "from gc import callbacks\n",
    "\n",
    "def model_vgg16():\n",
    "  IMG_SIZE = (224,224)\n",
    "  IMG_SHAPE = IMG_SIZE + (3,)\n",
    "\n",
    "  preprocess_input = tf.keras.applications.vgg16.preprocess_input\n",
    "\n",
    "  inputs = tf.keras.Input(shape=(IMG_SHAPE))\n",
    "  x = preprocess_input(inputs)\n",
    "\n",
    "  base_model = VGG16(input_shape=(IMG_SHAPE),\n",
    "                          include_top=False,\n",
    "                          weights='imagenet',\n",
    "                        input_tensor=x)\n",
    "\n",
    "  # base_model.trainable = False\n",
    "  base_model.trainable = True\n",
    "\n",
    "  if base_model.trainable:\n",
    "      # Let's take a look to see how many layers are in the base model\n",
    "    print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "    # Fine-tune from this layer onwards\n",
    "    fine_tune_at = -1\n",
    "    \n",
    "    number_frozen = 0\n",
    "\n",
    "    # Freeze all the layers before the `fine_tune_at` layer\n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "      layer.trainable = False\n",
    "      number_frozen = number_frozen + 1\n",
    "\n",
    "    # Un-freeze the BatchNorm layers\n",
    "    for layer in base_model.layers:\n",
    "      if \"BatchNormalization\" in layer.__class__.__name__:\n",
    "          layer.trainable = True\n",
    "          number_frozen = number_frozen - 1\n",
    "\n",
    "    print(str(number_frozen) + \" layers were frozen\")\n",
    "    print(str(len(base_model.layers) - number_frozen) + \" layers to fine-tune\")\n",
    "\n",
    "\n",
    "\n",
    "  # inputs = tf.keras.Input(shape=(IMG_SHAPE))\n",
    "  # x = preprocess_input(inputs)\n",
    "  # x = base_model(x, training=base_model.trainable)\n",
    "  # x = tf.keras.layers.AveragePooling2D()(base_model.output)\n",
    "  x = tf.keras.layers.Flatten()(base_model.output)\n",
    "  x = tf.keras.layers.Dropout(0.5)(x)\n",
    "  # x = tf.keras.layers.Dense(2048, activation=\"relu\")(x)\n",
    "  x = tf.keras.layers.Dense(100, activation=\"relu\")(x)\n",
    "  # x = tf.keras.layers.Dropout(0.5)(x)\n",
    "  x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "  # x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "  outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "  model = tf.keras.Model(base_model.input, outputs)\n",
    "\n",
    "\n",
    "  base_learning_rate = 0.0003\n",
    "  #import tensorflow_addons as tfa\n",
    "  opt = tf.optimizers.Adam(learning_rate=base_learning_rate, decay=base_learning_rate/50)\n",
    "\n",
    "\n",
    "  model.compile(optimizer=opt,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "  # model.compile(optimizer='SGD',\n",
    "  #               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "  #               metrics=['accuracy'])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "id": "TdObSP6hebI5"
   },
   "outputs": [],
   "source": [
    "#@title function to create inceptionv3\n",
    "\n",
    "from gc import callbacks\n",
    "\n",
    "def model_inceptionv3():\n",
    "  IMG_SIZE = (224,224)\n",
    "  IMG_SHAPE = IMG_SIZE + (3,)\n",
    "\n",
    "  preprocess_input = tf.keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "  inputs = tf.keras.Input(shape=(IMG_SHAPE))\n",
    "  x = preprocess_input(inputs)\n",
    "\n",
    "  base_model = InceptionV3(input_shape=(IMG_SHAPE),\n",
    "                          include_top=False,\n",
    "                          weights='imagenet',\n",
    "                        input_tensor=x)\n",
    "\n",
    "  # base_model.trainable = False\n",
    "  base_model.trainable = True\n",
    "\n",
    "  if base_model.trainable:\n",
    "      # Let's take a look to see how many layers are in the base model\n",
    "    print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "    # Fine-tune from this layer onwards\n",
    "    fine_tune_at = -1\n",
    "    \n",
    "    number_frozen = 0\n",
    "\n",
    "    # Freeze all the layers before the `fine_tune_at` layer\n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "      layer.trainable = False\n",
    "      number_frozen = number_frozen + 1\n",
    "\n",
    "    # Un-freeze the BatchNorm layers\n",
    "    for layer in base_model.layers:\n",
    "      if \"BatchNormalization\" in layer.__class__.__name__:\n",
    "          layer.trainable = True\n",
    "          number_frozen = number_frozen - 1\n",
    "\n",
    "    print(str(number_frozen) + \" layers were frozen\")\n",
    "    print(str(len(base_model.layers) - number_frozen) + \" layers to fine-tune\")\n",
    "\n",
    "\n",
    "\n",
    "  # inputs = tf.keras.Input(shape=(IMG_SHAPE))\n",
    "  # x = preprocess_input(inputs)\n",
    "  # x = base_model(x, training=base_model.trainable)\n",
    "  # x = tf.keras.layers.AveragePooling2D()(base_model.output)\n",
    "  x = tf.keras.layers.Flatten()(base_model.output)\n",
    "  x = tf.keras.layers.Dropout(0.5)(x)\n",
    "  # x = tf.keras.layers.Dense(2048, activation=\"relu\")(x)\n",
    "  x = tf.keras.layers.Dense(100, activation=\"relu\")(x)\n",
    "  # x = tf.keras.layers.Dropout(0.5)(x)\n",
    "  x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "  # x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "  outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "  model = tf.keras.Model(base_model.input, outputs)\n",
    "\n",
    "\n",
    "  base_learning_rate = 0.0003\n",
    "  #import tensorflow_addons as tfa\n",
    "  opt = tf.optimizers.Adam(learning_rate=base_learning_rate, decay=base_learning_rate/50)\n",
    "\n",
    "\n",
    "  model.compile(optimizer=opt,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "  # model.compile(optimizer='SGD',\n",
    "  #               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "  #               metrics=['accuracy'])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "id": "LPFI09C4ocP7"
   },
   "outputs": [],
   "source": [
    "#@title function to create xception\n",
    "\n",
    "from gc import callbacks\n",
    "\n",
    "def model_xception():\n",
    "  IMG_SIZE = (224,224)\n",
    "  IMG_SHAPE = IMG_SIZE + (3,)\n",
    "\n",
    "  preprocess_input = tf.keras.applications.xception.preprocess_input\n",
    "\n",
    "  inputs = tf.keras.Input(shape=(IMG_SHAPE))\n",
    "  x = preprocess_input(inputs)\n",
    "\n",
    "  base_model = Xception(input_shape=(IMG_SHAPE),\n",
    "                          include_top=False,\n",
    "                          weights='imagenet',\n",
    "                        input_tensor=x)\n",
    "\n",
    "  # base_model.trainable = False\n",
    "  base_model.trainable = True\n",
    "\n",
    "  if base_model.trainable:\n",
    "      # Let's take a look to see how many layers are in the base model\n",
    "    print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "    # Fine-tune from this layer onwards\n",
    "    fine_tune_at = -1\n",
    "    \n",
    "    number_frozen = 0\n",
    "\n",
    "    # Freeze all the layers before the `fine_tune_at` layer\n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "      layer.trainable = False\n",
    "      number_frozen = number_frozen + 1\n",
    "\n",
    "    # Un-freeze the BatchNorm layers\n",
    "    for layer in base_model.layers:\n",
    "      if \"BatchNormalization\" in layer.__class__.__name__:\n",
    "          layer.trainable = True\n",
    "          number_frozen = number_frozen - 1\n",
    "\n",
    "    print(str(number_frozen) + \" layers were frozen\")\n",
    "    print(str(len(base_model.layers) - number_frozen) + \" layers to fine-tune\")\n",
    "\n",
    "\n",
    "\n",
    "  # inputs = tf.keras.Input(shape=(IMG_SHAPE))\n",
    "  # x = preprocess_input(inputs)\n",
    "  # x = base_model(x, training=base_model.trainable)\n",
    "  # x = tf.keras.layers.AveragePooling2D()(base_model.output)\n",
    "  x = tf.keras.layers.Flatten()(base_model.output)\n",
    "  x = tf.keras.layers.Dropout(0.5)(x)\n",
    "  # x = tf.keras.layers.Dense(2048, activation=\"relu\")(x)\n",
    "  x = tf.keras.layers.Dense(100, activation=\"relu\")(x)\n",
    "  # x = tf.keras.layers.Dropout(0.5)(x)\n",
    "  x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "  # x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "  outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "  model = tf.keras.Model(base_model.input, outputs)\n",
    "\n",
    "\n",
    "  base_learning_rate = 0.0003\n",
    "  #import tensorflow_addons as tfa\n",
    "  opt = tf.optimizers.Adam(learning_rate=base_learning_rate, decay=base_learning_rate/50)\n",
    "\n",
    "\n",
    "  model.compile(optimizer=opt,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "  # model.compile(optimizer='SGD',\n",
    "  #               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "  #               metrics=['accuracy'])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-O6NSZuiYv9m",
    "outputId": "af8309e7-ed15-4260-e135-02d9352329e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
      "/content/drive/MyDrive/Colab Notebooks/Mestrado/bancos\n",
      "\u001b[0m\u001b[01;34mCOVID-CT\u001b[0m/  \u001b[01;34mmodels\u001b[0m/  \u001b[01;34msars-cov-2\u001b[0m/  \u001b[01;34msars-cov-2-multi\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "%cd '/content/drive/MyDrive/Colab Notebooks/Mestrado/bancos/'\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "yb1isYRfgZes"
   },
   "outputs": [],
   "source": [
    "imagePaths = list(paths.list_images('./sars-cov-2/0_non-COVID/')) + list(paths.list_images('./sars-cov-2/COVID/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJ05eJkburBc"
   },
   "source": [
    "# Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "T9_2XoUeE4hF",
    "outputId": "9bb8d7a5-25e3-4d15-8776-ea98e343b12e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "====== K Fold Validation step => 1/5 =======\n",
      "=========================================\n",
      "[INFO] creating a tf.data input pipeline..\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "83689472/83683744 [==============================] - 0s 0us/step\n",
      "83697664/83683744 [==============================] - 0s 0us/step\n",
      "Number of layers in the base model:  134\n",
      "93 layers were frozen\n",
      "41 layers to fine-tune\n",
      "\n",
      "---------- Training inceptionv3 ----------\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.73185, saving model to ./models/xception/model_1.h5\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 1s 0us/step\n",
      "87924736/87910968 [==============================] - 1s 0us/step\n",
      "Number of layers in the base model:  313\n",
      "218 layers were frozen\n",
      "95 layers to fine-tune\n",
      "\n",
      "---------- Training inceptionv3 ----------\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.63911, saving model to ./models/inceptionv3/model_1.h5\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 1s 0us/step\n",
      "58900480/58889256 [==============================] - 1s 0us/step\n",
      "Number of layers in the base model:  21\n",
      "20 layers were frozen\n",
      "1 layers to fine-tune\n",
      "\n",
      "---------- Training vgg16 ----------\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.89516, saving model to ./models/vgg16/model_1.h5\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 1s 0us/step\n",
      "94781440/94765736 [==============================] - 1s 0us/step\n",
      "Number of layers in the base model:  177\n",
      "122 layers were frozen\n",
      "55 layers to fine-tune\n",
      "\n",
      "---------- Training ResNet50 ----------\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-656bfd233a47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasetGen_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         validation_steps=VALIDATION_STEPS)\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./models/resnet50/model_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_get_operation_by_name_unsafe\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   4123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4124\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4125\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4127\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_operation_by_tf_operation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_oper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'gradient_tape/model/conv5_block2_3_bn/FusedBatchNormGradV3'"
     ]
    }
   ],
   "source": [
    "# Generator for kfold\n",
    "from tensorflow.data import AUTOTUNE\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_model_name(k):\n",
    "    return 'model_'+str(k)+'.h5'\n",
    "\n",
    "def load_images(imagePath):\n",
    "\t# read the image from disk, decode it, resize it, and scale the\n",
    "\t# pixels intensities to the range [0, 1]\n",
    "\timage = tf.io.read_file(imagePath)\n",
    "\timage = tf.image.decode_png(image, channels=3)\n",
    "\timage = tf.image.resize(image, (224, 224))\n",
    "\t# grab the label and encode it\n",
    "\tlabel = tf.strings.split(imagePath, os.path.sep)[-2]\n",
    "\toneHot = label == classNames\n",
    "\tencodedLabel = tf.argmax(oneHot)\n",
    "\t# return the image and the integer encoded label\n",
    "\treturn (image, oneHot)\n",
    " \n",
    "def augment_using_ops(images, labels):\n",
    "  images = tf.image.random_flip_left_right(images)\n",
    "  return (images, labels)\n",
    " \n",
    "IMG_SIZE = (224,224) \n",
    " # initialize batch size and number of steps\n",
    "BATCH_SIZE = 4\n",
    "# grab the list of images in our dataset directory and grab all\n",
    "# unique class names\n",
    "EPOCHS = 1\n",
    "\n",
    "#print(\"[INFO] loading image paths...\")\n",
    "imagePaths = list(paths.list_images('./sars-cov-2/0_non-COVID/')) + list(paths.list_images('./sars-cov-2/COVID/'))\n",
    "imagePaths = np.array(imagePaths)\n",
    "imagePaths_train, imagePaths_test = train_test_split(imagePaths, test_size=0.2, random_state=42)\n",
    "classNames = ['0_non-COVID', 'COVID']\n",
    "\n",
    "kf = KFold(n_splits = 2)\n",
    "i = 1                       \n",
    "VALIDATION_ACCURACY = {'resnet50':[], 'mobilenetv2':[], 'vgg16':[], 'inceptionv3':[], 'xception':[]}\n",
    "VALIDAITON_LOSS = {'resnet50':[], 'mobilenetv2':[], 'vgg16':[], 'inceptionv3':[], 'xception':[]}\n",
    "\n",
    "TEST_ACCURACY = {'resnet50':[], 'mobilenetv2':[], 'vgg16':[], 'inceptionv3':[], 'xception':[]}\n",
    "TEST_LOSS = {'resnet50':[], 'mobilenetv2':[], 'vgg16':[], 'inceptionv3':[], 'xception':[]}\n",
    "\n",
    "for train_index, val_index in kf.split(imagePaths_train):\n",
    "  print(\"=========================================\")\n",
    "  print(f\"====== K Fold Validation step => {i}/5 =======\")\n",
    "  print(\"=========================================\")\n",
    "  trainData = imagePaths_train[train_index]\n",
    "  valData = imagePaths_train[val_index]\n",
    "\n",
    "  # build the dataset and data input pipeline\n",
    "  print(\"[INFO] creating a tf.data input pipeline..\")\n",
    "  dataset_train = tf.data.Dataset.from_tensor_slices(trainData)\n",
    "  dataset_train = (dataset_train\n",
    "    .shuffle(1024)\n",
    "    .map(load_images, num_parallel_calls=AUTOTUNE)\n",
    "    .cache()\n",
    "    .repeat()\n",
    "    .batch(BATCH_SIZE)\n",
    "    .map(augment_using_ops, num_parallel_calls=AUTOTUNE)\n",
    "    .prefetch(AUTOTUNE)\n",
    "  )\n",
    "\n",
    "  datasetGen_train = iter(dataset_train)\n",
    "\n",
    "  dataset_val = tf.data.Dataset.from_tensor_slices(valData)\n",
    "  dataset_val = (dataset_val\n",
    "    .shuffle(1024)\n",
    "    .map(load_images, num_parallel_calls=AUTOTUNE)\n",
    "    .cache()\n",
    "    .repeat()\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    "  )\n",
    "\n",
    "  datasetGen_val = iter(dataset_val)\n",
    "\n",
    "  dataset_test = tf.data.Dataset.from_tensor_slices(imagePaths_test)\n",
    "  dataset_test = (dataset_test\n",
    "    .shuffle(1024)\n",
    "    .map(load_images, num_parallel_calls=AUTOTUNE)\n",
    "    .cache()\n",
    "    .repeat()\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTOTUNE)\n",
    "  )\n",
    "\n",
    "  datasetGen_test = iter(dataset_test) \n",
    "\n",
    "  training_images = len(trainData)\n",
    "  val_images = len(valData)\n",
    "\n",
    "  STEPS_PER_EPOCH = training_images // BATCH_SIZE\n",
    "  VALIDATION_STEPS = val_images // BATCH_SIZE\n",
    "\n",
    "  ### Xception\n",
    "\n",
    "  model = model_xception()\n",
    "\n",
    "  save_dir = './models/xception/'\n",
    "  checkpoint = tf.keras.callbacks.ModelCheckpoint(save_dir+get_model_name(i), \n",
    "\t\t\t\t\t\t\tmonitor='val_accuracy', verbose=1, \n",
    "\t\t\t\t\t\t\tsave_best_only=True, mode='max')\n",
    "  callbacks_list = [checkpoint]\n",
    "\n",
    "  print('\\n---------- Training inceptionv3 ----------')\n",
    "\n",
    "  history = model.fit(\n",
    "        datasetGen_train,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH,  \n",
    "        epochs=EPOCHS,\n",
    "        verbose=0,\n",
    "        callbacks=callbacks_list,\n",
    "        validation_data = datasetGen_val,\n",
    "        validation_steps=VALIDATION_STEPS)\n",
    "  \n",
    "  model.load_weights(\"./models/xception/model_\"+str(i)+\".h5\")\n",
    "  \n",
    "  results_val = model.evaluate(datasetGen_val, steps=VALIDATION_STEPS, verbose=0)\n",
    "  results_val = dict(zip(model.metrics_names,results_val))\n",
    "\n",
    "  results_test = model.evaluate(datasetGen_test, steps=len(imagePaths_test), verbose=0)\n",
    "  results_test = dict(zip(model.metrics_names,results_test))\n",
    "\n",
    "  VALIDATION_ACCURACY['xception'].append(results_val['accuracy'])\n",
    "  VALIDAITON_LOSS['xception'].append(results_val['accuracy'])\n",
    "\n",
    "  TEST_ACCURACY['xception'].append(results_test['accuracy'])\n",
    "  TEST_LOSS['xception'].append(results_test['accuracy'])\n",
    "\n",
    "  tf.keras.backend.clear_session()\n",
    "\n",
    "  ### InceptionV3\n",
    "\n",
    "  model = model_inceptionv3()\n",
    "\n",
    "  save_dir = './models/inceptionv3/'\n",
    "  checkpoint = tf.keras.callbacks.ModelCheckpoint(save_dir+get_model_name(i), \n",
    "\t\t\t\t\t\t\tmonitor='val_accuracy', verbose=1, \n",
    "\t\t\t\t\t\t\tsave_best_only=True, mode='max')\n",
    "  callbacks_list = [checkpoint]\n",
    "\n",
    "  print('\\n---------- Training inceptionv3 ----------')\n",
    "\n",
    "  history = model.fit(\n",
    "        datasetGen_train,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH,  \n",
    "        epochs=EPOCHS,\n",
    "        verbose=0,\n",
    "        callbacks=callbacks_list,\n",
    "        validation_data = datasetGen_val,\n",
    "        validation_steps=VALIDATION_STEPS)\n",
    "  \n",
    "  model.load_weights(\"./models/inceptionv3/model_\"+str(i)+\".h5\")\n",
    "  \n",
    "  results_val = model.evaluate(datasetGen_val, steps=VALIDATION_STEPS, verbose=0)\n",
    "  results_val = dict(zip(model.metrics_names,results_val))\n",
    "\n",
    "  results_test = model.evaluate(datasetGen_test, steps=len(imagePaths_test), verbose=0)\n",
    "  results_test = dict(zip(model.metrics_names,results_test))\n",
    "\n",
    "  VALIDATION_ACCURACY['inceptionv3'].append(results_val['accuracy'])\n",
    "  VALIDAITON_LOSS['inceptionv3'].append(results_val['accuracy'])\n",
    "\n",
    "  TEST_ACCURACY['inceptionv3'].append(results_test['accuracy'])\n",
    "  TEST_LOSS['inceptionv3'].append(results_test['accuracy'])\n",
    "\n",
    "  tf.keras.backend.clear_session()\n",
    "\n",
    "  ### VGG16\n",
    "\n",
    "  model = model_vgg16()\n",
    "\n",
    "  save_dir = './models/vgg16/'\n",
    "  checkpoint = tf.keras.callbacks.ModelCheckpoint(save_dir+get_model_name(i), \n",
    "\t\t\t\t\t\t\tmonitor='val_accuracy', verbose=1, \n",
    "\t\t\t\t\t\t\tsave_best_only=True, mode='max')\n",
    "  callbacks_list = [checkpoint]\n",
    "\n",
    "  print('\\n---------- Training vgg16 ----------')\n",
    "\n",
    "  history = model.fit(\n",
    "        datasetGen_train,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH,  \n",
    "        epochs=EPOCHS,\n",
    "        verbose=0,\n",
    "        callbacks=callbacks_list,\n",
    "        validation_data = datasetGen_val,\n",
    "        validation_steps=VALIDATION_STEPS)\n",
    "  \n",
    "  model.load_weights(\"./models/vgg16/model_\"+str(i)+\".h5\")\n",
    "  \n",
    "  results_val = model.evaluate(datasetGen_val, steps=VALIDATION_STEPS, verbose=0)\n",
    "  results_val = dict(zip(model.metrics_names,results_val))\n",
    "\n",
    "  results_test = model.evaluate(datasetGen_test, steps=len(imagePaths_test), verbose=0)\n",
    "  results_test = dict(zip(model.metrics_names,results_test))\n",
    "\n",
    "  VALIDATION_ACCURACY['vgg16'].append(results_val['accuracy'])\n",
    "  VALIDAITON_LOSS['vgg16'].append(results_val['accuracy'])\n",
    "\n",
    "  TEST_ACCURACY['vgg16'].append(results_test['accuracy'])\n",
    "  TEST_LOSS['vgg16'].append(results_test['accuracy'])\n",
    "\n",
    "  tf.keras.backend.clear_session()\n",
    "\n",
    "  ### ResNet50\n",
    "\n",
    "  model = model_resnet50()\n",
    "\n",
    "  save_dir = './models/resnet50/'\n",
    "  checkpoint = tf.keras.callbacks.ModelCheckpoint(save_dir+get_model_name(i), \n",
    "\t\t\t\t\t\t\tmonitor='val_accuracy', verbose=1, \n",
    "\t\t\t\t\t\t\tsave_best_only=True, mode='max')\n",
    "  callbacks_list = [checkpoint]\n",
    "\n",
    "  print('\\n---------- Training ResNet50 ----------')\n",
    "\n",
    "  history = model.fit(\n",
    "        datasetGen_train,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH,  \n",
    "        epochs=EPOCHS,\n",
    "        verbose=0,\n",
    "        callbacks=callbacks_list,\n",
    "        validation_data = datasetGen_val,\n",
    "        validation_steps=VALIDATION_STEPS)\n",
    "  \n",
    "  model.load_weights(\"./models/resnet50/model_\"+str(i)+\".h5\")\n",
    "  \n",
    "  results_val = model.evaluate(datasetGen_val, steps=VALIDATION_STEPS, verbose=0)\n",
    "  results_val = dict(zip(model.metrics_names,results_val))\n",
    "\n",
    "  results_test = model.evaluate(datasetGen_test, steps=len(imagePaths_test), verbose=0)\n",
    "  results_test = dict(zip(model.metrics_names,results_test))\n",
    "\n",
    "  VALIDATION_ACCURACY['resnet50'].append(results_val['accuracy'])\n",
    "  VALIDAITON_LOSS['resnet50'].append(results_val['accuracy'])\n",
    "\n",
    "  TEST_ACCURACY['resnet50'].append(results_test['accuracy'])\n",
    "  TEST_LOSS['resnet50'].append(results_test['accuracy'])\n",
    "\n",
    "  tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "  ### MobileNet\n",
    "  model = model_mobilenetv2()\n",
    "\n",
    "  save_dir = './models/mobilenetv2/'\n",
    "  checkpoint = tf.keras.callbacks.ModelCheckpoint(save_dir+get_model_name(i), \n",
    "\t\t\t\t\t\t\tmonitor='val_accuracy', verbose=1, \n",
    "\t\t\t\t\t\t\tsave_best_only=True, mode='max')\n",
    "  callbacks_list = [checkpoint]\n",
    "\n",
    "  print('\\n---------- Training MobileNetV2 ----------')\n",
    "\n",
    "  history = model.fit(\n",
    "        datasetGen_train,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH,  \n",
    "        epochs=EPOCHS,\n",
    "        verbose=0,\n",
    "        callbacks=callbacks_list,\n",
    "        validation_data = datasetGen_val,\n",
    "        validation_steps=VALIDATION_STEPS)\n",
    "  \n",
    "  model.load_weights(\"./models/mobilenetv2/model_\"+str(i)+\".h5\")\n",
    "  \n",
    "  results_val = model.evaluate(datasetGen_val, steps=VALIDATION_STEPS, verbose=0)\n",
    "  results_val = dict(zip(model.metrics_names,results_val))\n",
    "\n",
    "  results_test = model.evaluate(datasetGen_test, steps=len(imagePaths_test), verbose=0)\n",
    "  results_test = dict(zip(model.metrics_names,results_test))\n",
    "\n",
    "  VALIDATION_ACCURACY['mobilenetv2'].append(results_val['accuracy'])\n",
    "  VALIDAITON_LOSS['mobilenetv2'].append(results_val['accuracy'])\n",
    "\n",
    "  TEST_ACCURACY['mobilenetv2'].append(results_test['accuracy'])\n",
    "  TEST_LOSS['mobilenetv2'].append(results_test['accuracy'])\n",
    "\n",
    "  tf.keras.backend.clear_session()\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "  i = i+1\n",
    "\n",
    "#print(VALIDATION_ACCURACY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMFxQ__IukDa"
   },
   "source": [
    "# ResNet50 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nxFryL04zqna",
    "outputId": "aff7f3c2-4adb-4ebe-98f2-79d0c3c160f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(497, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load images and Convert them to a Numpy array\n",
    "imgs = []\n",
    "labels = []\n",
    "i = 0\n",
    "\n",
    "for img in imagePaths_test:\n",
    "  label = img.split('/')[-2]\n",
    "  img = load_img(img, target_size=(224, 224))\n",
    "  imgs.append(np.array(img))\n",
    "\n",
    "  \n",
    "  if label == 'COVID':\n",
    "    labels.append([0, 1.0])\n",
    "    i = i + 1\n",
    "  else:\n",
    "    labels.append([1.0, 0])\n",
    " \n",
    "testX = np.asarray(imgs)\n",
    "\n",
    "testY = np.array(labels)\n",
    "#preprocess_input = tf.keras.applications.resnet50.preprocess_input\n",
    "#testX = preprocess_input(images)\n",
    "print(np.shape(testX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ds50xZAkF2F",
    "outputId": "4f4c2d3b-a0a1-46c2-9fef-d8c7e5a6ba3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers in the base model:  177\n",
      "122 layers were frozen\n",
      "55 layers to fine-tune\n",
      "497/497 [==============================] - 13s 23ms/step - loss: 0.1335 - accuracy: 0.9618\n",
      "0.9617705941200256\n"
     ]
    }
   ],
   "source": [
    "### ResNet50\n",
    "model = model_resnet50()\n",
    "model.load_weights(\"./models/resnet50/model_\"+str(np.argmax(TEST_ACCURACY[\"resnet50\"])+1)+\".h5\")\n",
    "results_test = model.evaluate(datasetGen_test, steps=len(imagePaths_test))\n",
    "results_test = dict(zip(model.metrics_names,results_test))\n",
    "print(results_test['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "ev8BZprrDsQ4",
    "outputId": "f9bfba90-c36e-4212-c2ab-4cdffc76a500"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non covid       0.97      0.95      0.96       238\n",
      "       covid       0.95      0.97      0.96       259\n",
      "\n",
      "    accuracy                           0.96       497\n",
      "   macro avg       0.96      0.96      0.96       497\n",
      "weighted avg       0.96      0.96      0.96       497\n",
      "\n",
      "[[226  12]\n",
      " [  8 251]]\n",
      "acc: 0.9598\n",
      "sensitivity: 0.9496\n",
      "specificity: 0.9691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd1957e0090>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### ResNet50\n",
    "# make predictions on the testing set\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predIdxs = model.predict(testX, batch_size=1)\n",
    "\n",
    "#print(predIdxs)\n",
    "\n",
    "# for each image in the testing set we need to find the index of the\n",
    "# label with corresponding largest predicted probability\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "\n",
    "#testY = np.array([[0, 1.0], [0, 1.0], [1.0, 0]])\n",
    "\n",
    "# show a nicely formatted classification report\n",
    "print(classification_report(testY.argmax(axis=1), predIdxs,\n",
    "\ttarget_names=['non covid', 'covid']))\n",
    "\n",
    "# vp fp\n",
    "# fn vn \n",
    "\n",
    "# compute the confusion matrix and and use it to derive the raw\n",
    "# accuracy, sensitivity, and specificity\n",
    "cm = confusion_matrix(testY.argmax(axis=1), predIdxs)\n",
    "total = sum(sum(cm))\n",
    "acc = (cm[0, 0] + cm[1, 1]) / total\n",
    "sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "\n",
    "# show the confusion matrix, accuracy, sensitivity, and specificity\n",
    "print(cm)\n",
    "print(\"acc: {:.4f}\".format(acc))\n",
    "print(\"sensitivity: {:.4f}\".format(sensitivity))\n",
    "print(\"specificity: {:.4f}\".format(specificity))\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "N = EPOCHS\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on COVID-19 Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IBrBmOWuvUM"
   },
   "source": [
    "# MobileNetV2 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AyOndT1cKNok"
   },
   "outputs": [],
   "source": [
    "### MobileNetV2\n",
    "model = model_mobilenetv2()\n",
    "model.load_weights(\"./models/mobilenetv2/model_\"+str(np.argmax(TEST_ACCURACY[\"mobilenetv2\"])+1)+\".h5\")\n",
    "results_test = model.evaluate(datasetGen_test, steps=len(imagePaths_test))\n",
    "results_test = dict(zip(model.metrics_names,results_test))\n",
    "print(results_test['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kz-oCX16KRFU"
   },
   "outputs": [],
   "source": [
    "### MobileNetV2\n",
    "# make predictions on the testing set\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predIdxs = model.predict(testX, batch_size=1)\n",
    "\n",
    "#print(predIdxs)\n",
    "\n",
    "# for each image in the testing set we need to find the index of the\n",
    "# label with corresponding largest predicted probability\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "\n",
    "#testY = np.array([[0, 1.0], [0, 1.0], [1.0, 0]])\n",
    "\n",
    "# show a nicely formatted classification report\n",
    "print(classification_report(testY.argmax(axis=1), predIdxs,\n",
    "\ttarget_names=['non covid', 'covid']))\n",
    "\n",
    "# vp fp\n",
    "# fn vn \n",
    "\n",
    "# compute the confusion matrix and and use it to derive the raw\n",
    "# accuracy, sensitivity, and specificity\n",
    "cm = confusion_matrix(testY.argmax(axis=1), predIdxs)\n",
    "total = sum(sum(cm))\n",
    "acc = (cm[0, 0] + cm[1, 1]) / total\n",
    "sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "\n",
    "# show the confusion matrix, accuracy, sensitivity, and specificity\n",
    "print(cm)\n",
    "print(\"acc: {:.4f}\".format(acc))\n",
    "print(\"sensitivity: {:.4f}\".format(sensitivity))\n",
    "print(\"specificity: {:.4f}\".format(specificity))\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "N = EPOCHS\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on COVID-19 Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hr4tPcdcuykb"
   },
   "source": [
    "# VGG16 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HpxMugJfKUuD"
   },
   "outputs": [],
   "source": [
    "### VGG16\n",
    "model = model_vgg16()\n",
    "model.load_weights(\"./models/vgg16/model_\"+str(np.argmax(TEST_ACCURACY[\"vgg16\"])+1)+\".h5\")\n",
    "results_test = model.evaluate(datasetGen_test, steps=len(imagePaths_test))\n",
    "results_test = dict(zip(model.metrics_names,results_test))\n",
    "print(results_test['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mMbWYU0ddsoj"
   },
   "outputs": [],
   "source": [
    "### VGG16\n",
    "# make predictions on the testing set\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predIdxs = model.predict(testX, batch_size=1)\n",
    "\n",
    "#print(predIdxs)\n",
    "\n",
    "# for each image in the testing set we need to find the index of the\n",
    "# label with corresponding largest predicted probability\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "\n",
    "#testY = np.array([[0, 1.0], [0, 1.0], [1.0, 0]])\n",
    "\n",
    "# show a nicely formatted classification report\n",
    "print(classification_report(testY.argmax(axis=1), predIdxs,\n",
    "\ttarget_names=['non covid', 'covid']))\n",
    "\n",
    "# vp fp\n",
    "# fn vn \n",
    "\n",
    "# compute the confusion matrix and and use it to derive the raw\n",
    "# accuracy, sensitivity, and specificity\n",
    "cm = confusion_matrix(testY.argmax(axis=1), predIdxs)\n",
    "total = sum(sum(cm))\n",
    "acc = (cm[0, 0] + cm[1, 1]) / total\n",
    "sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "\n",
    "# show the confusion matrix, accuracy, sensitivity, and specificity\n",
    "print(cm)\n",
    "print(\"acc: {:.4f}\".format(acc))\n",
    "print(\"sensitivity: {:.4f}\".format(sensitivity))\n",
    "print(\"specificity: {:.4f}\".format(specificity))\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "N = EPOCHS\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on COVID-19 Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5CLtj9acu1O7"
   },
   "source": [
    "# InceptionV3 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSl_HPdxf4ZY"
   },
   "outputs": [],
   "source": [
    "### InceptionV3\n",
    "model = model_inceptionv3()\n",
    "model.load_weights(\"./models/inceptionv3/model_\"+str(np.argmax(TEST_ACCURACY[\"inceptionv3\"])+1)+\".h5\")\n",
    "results_test = model.evaluate(datasetGen_test, steps=len(imagePaths_test))\n",
    "results_test = dict(zip(model.metrics_names,results_test))\n",
    "print(results_test['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7tSWNtaf-TV"
   },
   "outputs": [],
   "source": [
    "### InceptionV3\n",
    "# make predictions on the testing set\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predIdxs = model.predict(testX, batch_size=1)\n",
    "\n",
    "#print(predIdxs)\n",
    "\n",
    "# for each image in the testing set we need to find the index of the\n",
    "# label with corresponding largest predicted probability\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "\n",
    "#testY = np.array([[0, 1.0], [0, 1.0], [1.0, 0]])\n",
    "\n",
    "# show a nicely formatted classification report\n",
    "print(classification_report(testY.argmax(axis=1), predIdxs,\n",
    "\ttarget_names=['non covid', 'covid']))\n",
    "\n",
    "# vp fp\n",
    "# fn vn \n",
    "\n",
    "# compute the confusion matrix and and use it to derive the raw\n",
    "# accuracy, sensitivity, and specificity\n",
    "cm = confusion_matrix(testY.argmax(axis=1), predIdxs)\n",
    "total = sum(sum(cm))\n",
    "acc = (cm[0, 0] + cm[1, 1]) / total\n",
    "sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "\n",
    "# show the confusion matrix, accuracy, sensitivity, and specificity\n",
    "print(cm)\n",
    "print(\"acc: {:.4f}\".format(acc))\n",
    "print(\"sensitivity: {:.4f}\".format(sensitivity))\n",
    "print(\"specificity: {:.4f}\".format(specificity))\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "N = EPOCHS\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on COVID-19 Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFVaRjE9u3-L"
   },
   "source": [
    "# Xception Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2T58ovUpS1F"
   },
   "outputs": [],
   "source": [
    "### Xception\n",
    "model = model_xception()\n",
    "model.load_weights(\"./models/xception/model_\"+str(np.argmax(TEST_ACCURACY[\"xception\"])+1)+\".h5\")\n",
    "results_test = model.evaluate(datasetGen_test, steps=len(imagePaths_test))\n",
    "results_test = dict(zip(model.metrics_names,results_test))\n",
    "print(results_test['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "seQ8JZvYpS4E"
   },
   "outputs": [],
   "source": [
    "### Xception\n",
    "# make predictions on the testing set\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predIdxs = model.predict(testX, batch_size=1)\n",
    "\n",
    "#print(predIdxs)\n",
    "\n",
    "# for each image in the testing set we need to find the index of the\n",
    "# label with corresponding largest predicted probability\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "\n",
    "#testY = np.array([[0, 1.0], [0, 1.0], [1.0, 0]])\n",
    "\n",
    "# show a nicely formatted classification report\n",
    "print(classification_report(testY.argmax(axis=1), predIdxs,\n",
    "\ttarget_names=['non covid', 'covid']))\n",
    "\n",
    "# vp fp\n",
    "# fn vn \n",
    "\n",
    "# compute the confusion matrix and and use it to derive the raw\n",
    "# accuracy, sensitivity, and specificity\n",
    "cm = confusion_matrix(testY.argmax(axis=1), predIdxs)\n",
    "total = sum(sum(cm))\n",
    "acc = (cm[0, 0] + cm[1, 1]) / total\n",
    "sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "\n",
    "# show the confusion matrix, accuracy, sensitivity, and specificity\n",
    "print(cm)\n",
    "print(\"acc: {:.4f}\".format(acc))\n",
    "print(\"sensitivity: {:.4f}\".format(sensitivity))\n",
    "print(\"specificity: {:.4f}\".format(specificity))\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "N = EPOCHS\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on COVID-19 Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTFXWVzpu6hf"
   },
   "source": [
    "# Ensemble Results - Integrated Stacking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LIrQRNkuq7qX",
    "outputId": "e21cf788-249d-4044-9a3e-2efddd208841"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers in the base model:  177\n",
      "122 layers were frozen\n",
      "55 layers to fine-tune\n",
      "Number of layers in the base model:  156\n",
      "48 layers were frozen\n",
      "108 layers to fine-tune\n",
      "Number of layers in the base model:  21\n",
      "20 layers were frozen\n",
      "1 layers to fine-tune\n",
      "Number of layers in the base model:  313\n",
      "218 layers were frozen\n",
      "95 layers to fine-tune\n",
      "Number of layers in the base model:  134\n",
      "93 layers were frozen\n",
      "41 layers to fine-tune\n"
     ]
    }
   ],
   "source": [
    "# https://machinelearningmastery.com/stacking-ensemble-for-deep-learning-neural-networks/\n",
    "\n",
    "resnet50 = model_resnet50()\n",
    "#resnet50.load_weights(\"./models/resnet50/model_7.h5\")\n",
    "resnet50.load_weights(\"./models/resnet50/model_\"+str(np.argmax(TEST_ACCURACY[\"resnet50\"])+1)+\".h5\")\n",
    "\n",
    "mobilenetv2 = model_mobilenetv2()\n",
    "#mobilenetv2.load_weights(\"./models/mobilenetv2/model_8.h5\")\n",
    "model.load_weights(\"./models/mobilenetv2/model_\"+str(np.argmax(TEST_ACCURACY[\"mobilenetv2\"])+1)+\".h5\")\n",
    "\n",
    "vgg16 = model_vgg16()\n",
    "#vgg16.load_weights(\"./models/vgg16/model_5.h5\")\n",
    "model.load_weights(\"./models/vgg16/model_\"+str(np.argmax(TEST_ACCURACY[\"vgg16\"])+1)+\".h5\")\n",
    "\n",
    "inceptionv3 = model_inceptionv3()\n",
    "#inceptionv3.load_weights(\"./models/inceptionv3/model_5.h5\")\n",
    "model.load_weights(\"./models/inceptionv3/model_\"+str(np.argmax(TEST_ACCURACY[\"inceptionv3\"])+1)+\".h5\")\n",
    "\n",
    "xception = model_xception()\n",
    "#xception.load_weights(\"./models/xception/model_5.h5\")\n",
    "model.load_weights(\"./models/xception/model_\"+str(np.argmax(TEST_ACCURACY[\"xception\"])+1)+\".h5\")\n",
    "\n",
    "members = [resnet50, mobilenetv2, vgg16, inceptionv3, xception]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "MfznD6ir1iHz"
   },
   "outputs": [],
   "source": [
    "from keras.layers.merge import concatenate\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "\n",
    "# define stacked model from multiple member input models\n",
    "def define_stacked_model(members):\n",
    "\t# update all layers in all models to not be trainable\n",
    "\tfor i in range(len(members)):\n",
    "\t\tmodel = members[i]\n",
    "\t\tfor layer in model.layers:\n",
    "\t\t\t# make not trainable\n",
    "\t\t\tlayer.trainable = False\n",
    "\t\t\t# rename to avoid 'unique layer name' issue\n",
    "\t\t\tlayer._name = 'ensemble_' + str(i+1) + '_' + layer.name\n",
    "\t# define multi-headed input\n",
    "\tensemble_visible = [model.input for model in members]\n",
    "\t# concatenate merge output from each model\n",
    "\tensemble_outputs = [model.output for model in members]\n",
    "\tmerge = concatenate(ensemble_outputs)\n",
    "\thidden = Dense(10, activation='relu')(merge)\n",
    "\toutput = Dense(2, activation='softmax')(hidden)\n",
    "\tmodel = Model(inputs=ensemble_visible, outputs=output)\n",
    "\t# plot graph of ensemble\n",
    "\tplot_model(model, show_shapes=True, to_file='model_graph.png')\n",
    "\t# compile\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "Y8zOx08P2iOU"
   },
   "outputs": [],
   "source": [
    "stacked_model = define_stacked_model(members)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "rziy6Lgs9lPy"
   },
   "outputs": [],
   "source": [
    "# Load images and Convert them to a Numpy array\n",
    "imgs = []\n",
    "labels = []\n",
    "i = 0\n",
    "\n",
    "for img in imagePaths_train:\n",
    "  label = img.split('/')[-2]\n",
    "  img = load_img(img, target_size=(224, 224))\n",
    "  imgs.append(np.array(img))\n",
    "\n",
    "  \n",
    "  if label == 'COVID':\n",
    "    labels.append([0, 1.0])\n",
    "    i = i + 1\n",
    "  else:\n",
    "    labels.append([1.0, 0])\n",
    " \n",
    "trainX = np.asarray(imgs)\n",
    "\n",
    "trainY = np.array(labels)\n",
    "\n",
    "# Load images and Convert them to a Numpy array\n",
    "imgs = []\n",
    "labels = []\n",
    "i = 0\n",
    "\n",
    "for img in imagePaths_test:\n",
    "  label = img.split('/')[-2]\n",
    "  img = load_img(img, target_size=(224, 224))\n",
    "  imgs.append(np.array(img))\n",
    "\n",
    "  \n",
    "  if label == 'COVID':\n",
    "    labels.append([0, 1.0])\n",
    "    i = i + 1\n",
    "  else:\n",
    "    labels.append([1.0, 0])\n",
    " \n",
    "testX = np.asarray(imgs)\n",
    "\n",
    "testY = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TOh5QpYn4iZG",
    "outputId": "f1bae9f3-16b6-4ea4-c2ab-713ffdbe4db5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 27s 432ms/step - loss: 0.0538 - accuracy: 0.9879\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcf4c6b1150>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [trainX for _ in range(len(stacked_model.input))]\n",
    "\n",
    "stacked_model.fit(X, trainY, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "zGypObH1-pF_",
    "outputId": "fbd3cd0c-ef95-4980-a161-67e54e323a82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non covid       0.98      0.97      0.97       238\n",
      "       covid       0.97      0.98      0.98       259\n",
      "\n",
      "    accuracy                           0.97       497\n",
      "   macro avg       0.97      0.97      0.97       497\n",
      "weighted avg       0.97      0.97      0.97       497\n",
      "\n",
      "[[230   8]\n",
      " [  5 254]]\n",
      "acc: 0.9738\n",
      "sensitivity: 0.9664\n",
      "specificity: 0.9807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fcf4c81f3d0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [testX for _ in range(len(stacked_model.input))]\n",
    "\n",
    "### Ensemble\n",
    "# make predictions on the testing set\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predIdxs = stacked_model.predict(X, verbose=0)\n",
    "\n",
    "#print(predIdxs)\n",
    "\n",
    "# for each image in the testing set we need to find the index of the\n",
    "# label with corresponding largest predicted probability\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "\n",
    "#testY = np.array([[0, 1.0], [0, 1.0], [1.0, 0]])\n",
    "\n",
    "# show a nicely formatted classification report\n",
    "print(classification_report(testY.argmax(axis=1), predIdxs,\n",
    "\ttarget_names=['non covid', 'covid']))\n",
    "\n",
    "# vp fp\n",
    "# fn vn \n",
    "\n",
    "# compute the confusion matrix and and use it to derive the raw\n",
    "# accuracy, sensitivity, and specificity\n",
    "cm = confusion_matrix(testY.argmax(axis=1), predIdxs)\n",
    "total = sum(sum(cm))\n",
    "acc = (cm[0, 0] + cm[1, 1]) / total\n",
    "sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "\n",
    "# show the confusion matrix, accuracy, sensitivity, and specificity\n",
    "print(cm)\n",
    "print(\"acc: {:.4f}\".format(acc))\n",
    "print(\"sensitivity: {:.4f}\".format(sensitivity))\n",
    "print(\"specificity: {:.4f}\".format(specificity))\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "N = EPOCHS\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on COVID-19 Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QRn6Vmcr-Mpt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "uJ05eJkburBc",
    "9IBrBmOWuvUM",
    "hr4tPcdcuykb",
    "5CLtj9acu1O7",
    "QFVaRjE9u3-L"
   ],
   "name": "sars-cov-2-binary_testes_mestrado_v2_26-07-2022.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
